官网：http://kafka.apache.org/
Kafka教程：
	安装：https://www.cnblogs.com/qingyunzong/p/9005062.html
	介绍：
		https://www.cnblogs.com/qingyunzong/p/9004509.html
		https://www.orchome.com/kafka/index
	架构：https://www.cnblogs.com/qingyunzong/p/9004593.html
	高可用：https://www.cnblogs.com/qingyunzong/p/9004703.html
	Kafka在zookeeper中的存储：https://www.cnblogs.com/qingyunzong/p/9007107.html



## 介绍：

	Kafka是一个分布式消息系统。
	Kafka是一个多分区、多副本且基于 ZooKeeper协调的分布式消息系统。

### 特性：
		目前Kafka已经定位为一个分布式流式处理平台，
		它以高吞吐、
			在一台普通的服务器上既可以达到10W/s的吞吐速率；？
		可持久化、
			快速持久化，可以在O(1)的系统开销下进行消息持久化；？
		可水平扩展、
			完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；
		支持流数据处理等多种特性而被广泛使用。
			？
### 应用：
		1、消息系统：
			Kafka 和传统的消息系统（也称作消息中间件）都具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。与此同时，Kafka 还提供了大多数消息系统难以实现的消息顺序性保障及回溯消费的功能。
		2、存储系统：
			Kafka 把消息持久化到磁盘，相比于其他基于内存存储的系统而言，有效地降低了数据丢失的风险。也正是得益于Kafka 的消息持久化功能和多副本机制，我们可以把Kafka作为长期的数据存储系统来使用，只需要把对应的数据保留策略设置为“永久”或启用主题的日志压缩功能即可。
		3、流式处理平台：
			Kafka 不仅为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、连接、变换和聚合等各类操作
	附：
		Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。
			？为什么rabbitMq称为重量级

## Kafka基本概念：

> https://www.cnblogs.com/zjdxr-up/p/16104558.html

### Kafka 体系架构：
	1. 一个典型的 Kafka 体系架构包括 ，若干 Producer、若干 Broker、若干 Consumer，以及一个ZooKeeper集群。
	
	2. Producer将消息发送到Broker，Broker负责将收到的消息存储到磁盘中，而Consumer负责从Broker订阅并消费消息
	附：
		其中ZooKeeper是Kafka用来负责集群元数据的管理、控制器的选举等操作的。。
### Broker 服务代理节点
	对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务实例。
		附大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。
		一个或多个Broker组成了一个Kafka集群。
		
	broker作用：
		broker 上存储了消息。
#### Topic：主题
		Kafka中的消息以主题为单位进行归类。
		生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。
		附：
			主题是一个逻辑上的概念。
			用户操作消息的时候，只需指定消息的Topic，即可生产或消费数据，而不必关心数据存于何处。
#### Partition：分区
	主题由为一个或多个分区组成。分区是实际存在的。
	1. 主题的分区可以分布在不同的服务器（broker）上。
	？如何决定分区在那个节点上了。自动分配还是手动配置
	2. 每一条消息被发送到 broker 之前，会根据分区规则选择存储到哪个具体的分区。 
	附：如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中。 
	附：
		1.如果一个主题只对应一个文件，那么这个文件所在的机器I/O将会成为这个主题的性能瓶颈，而分区解决了这个问题。
		2.在创建主题的时候可以通过指定的参数来设置分区的个数，当然也可以在主题创建完成之后去修改分区的数量，通过增加分区的数量可以实现水平扩展。
##### 附：partition 分区构成：
```txt
分区在存储层面可以看作一个可追加的日志文件。
偏移量：
	1.消息在被追加到分区日志文件的时候会分配一个特定的偏移量（offset）。
	2.offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。
附：
	1.每个partition中的数据又使用多个segment文件存储。？
	2.在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。
	3.如果topic有多个partition，消费数据时就不能保证数据的顺序。
```
##### 附：partition  副本：
	1.Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。
		1.同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样）。
	    2.副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。
	    3.副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。
	2.副本的同步机制：
		消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。(这个范围可以通过参数进行配置。)	
		1.AR: 分区中的所有副本统称为AR（Assigned Replicas）。
		2.ISR:
			所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。ISR与HW和LEO有着紧密的关系。
		3.OSR: 
			与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR (Out-of-Sync Replicas ），由 此可见， AR=ISR+OSR 。在正常情况下，所有 的 follower 副本都应该与 leader 副本保持一定程度 的同步，即 AR=ISR, OSR 集合为空。leader 副本负 责维护和跟踪 ISR 集合中所有 follower 副本 的滞后状态， 当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除 。 如果 OSR 集合中有 follower 副本 “追上’了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合 。 
		4.HW： HW是High Watermark的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。
			LEO： LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的offset。	
			Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。
			同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。
			？
				那kafka生产端如何判断是成功提交了

![分区中各偏移量说明](http://rfy18y9f3.hn-bkt.clouddn.com/GJ4)5})3HSJ2$$)DRUPS0YF.png)

### Producer 生成者

		生产者即数据的发布者。
		该角色将消息发布到Kafka的topic中。
		附：
			生产者发送的消息，随机存储到一个partition中。当然生产者也可以指定数据存储的partition。
### Consumer 消费者
		消费者可以从broker中读取数据。

## 客户端开发

> https://www.cnblogs.com/zjdxr-up/p/16110187.html

### 	1.java实现消息发送：

#### 1.1 客户端开发一个正常的生产逻辑需要具备以下几个步骤：
​	（1）配置生产者客户端参数及创建相应的生产者实例。
​	（2）构建待发送的消息。
​	（3）发送消息。
​	（4）关闭生产者实例。

#### 1.2 java kafka进行消息生产发送代码示例：

```java
public class KafkaProducerAnalysis {
    public static final String brokerList = "localhost:9092";
    public static final String topic = "topic-demo";
    public static Properties initConfig() (
         Properties props = new Properties();
         props.put("bootstrap.servers", brokerList);
    　　  props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
 　　 　  props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
　　　　  properties. put ("client. id", "producer. client. id. demo");
         return props;
    }
    public static void main(String[] args) {
        //1.
        Properties props = initConfig();
        //KafkaProducer是线程安全的， 可以在多个线程中共享单个KafkaProducer实例?，也 可以将KafkaProducer实例进行池化来供其他线程调用。
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        //2.
        ProducerRecord<String,String> record = new ProducerRecord<> (topic, "hello, Kafka1 ");
        try {
            //3.
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            //4.
        }
    }
 }
```



#### 1.3 消息对象ProducerRecord :

 	构建的消息对象ProducerRecord, 它并不是单纯意义上的消息，它包含了多个属性 

```java
public class ProducerRecord<K, V> {
        private final String topic; //主题
        private final Integer partition; //分区号
    	//headers字段是消息的头部，它大多用来设定 一些与应用相关的信息，如无需要也可以不用设置。
        private final Headers headers; //消息头部
    	//key是用来指定消息的键， 它不仅是消息的附加信息，还可以用来计算分区号进而可以让消息发往特定的分区。
　　　　 //key可以让消息再进行二次归类， 同一个key的消息会被划分到同 一个分区中
    	//有key的消息还可以支持日志压缩的功能
        private final K key; //键
    	//value是指消息体，一般不为空，如果为空则表示特定的消息-墓碑消息;
        private final V value; //值
    	//timestamp是指消息的时间戳，它有 CreateTime 和 LogAppendTime 两种类型，前者表示消息创建的时间，后者表示消息追加到日志文件的时间.
        private final Long timestamp; //消息的时间戳
　　//省略其他成员方法和构造方法
}
```



### 2. 发送消息的三种模式及实现区别

**1. 发送消息主要有三种模式：** 

​	发后即忘(fire-and-forget)、同步(sync)及异步Casync)。

1. 发后即忘: 它只管往Kafka中发送消息而并不关心消息是否正确到达。在大多数情况下，这种发送方式没有什么问题，不过在某些时候（比如发生不可重试异常时）会造成消息的丢失?。 这种发送方式的性能最高， 可靠性也最差。

2. 同步：

   1. KafkaProducer的 send()方法并非是void类型， 而是Future<RecordMetadata>类型， send()方法有2个重载方法，具体定义如下：

      ```
      public Future<RecordMetadata> send(ProducerRecord<K, V> record)
      public Future<RecordMetadata> send(ProducerRecord<K, V> record,Callback callback)
      ```

   2. 实现同步的发送方式， 可以利用send()返回的 Future 对象实现:


```java
try {
    Future<RecordMetadata> future = producer.send(record);
    //示例中在执行send()方法之后直接链式调用了get()方法来阻塞等待Kaflca的响应，直到消息发送成功， 或者发生异常。 
    RecordMetadata metadata= future.get();
    //返回的RecordMetadata对象里包含了消息的一些元数据信息，比如当前消息的主题、分区号、分区中的偏移量(offset)、 时间戳等。
    System.out.println(metadata.topic() + "-" +metadata.partition() + ":" + metadata.offset());
    } catch (ExecutionException I InterruptedException e) {
    e.printStackTrace () ;
}
```

3. 异步 

   send()方法本身就是异步的

   ```
   public Future<RecordMetadata> send(ProducerRecord<K, V> record,Callback callback)
   ```

### 3. Kafka生产者客户端架构

​                  ![](http://qiniu.58xuejia.cn/Kafka生产者客户端架构.png)

1. 消息在通过send( )方法发往broker 的过程中，有可能需要经过拦截器(Interceptor)、 序列化器(Serializer)和分区器(Parttitioner)的一系列作用之后才能被真正地发往 broker。
2. 拦截器一般不是必需的， 而序列化器是必需的。分区器则看情况是否应用。

#### **拦截器**

　　生产者拦截器既可以用 来在消息发送前做一些准备工作 ，比如按照某个规则过滤不符合要求的消 息、修改消 息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。

　　生产者拦截器 的 使用 也 很方便，主要是自定义实现org .apache.kafka. clients. producer.Producerlnterceptor 接口。ProducerInterceptor 接 口中包含 3 个方法 ：

```java
public ProducerRecord<K, V> onSend (ProducerRecord<K, V> record);
public void onAcknowledgement(RecordMetadata metadata, Excepti on exception );
public void close() ;
```

 　KafkaProducer 在将消息序列化和计算分区之前会调用 生产者拦截器 的 onSend（）方法来对消息进行相应 的定制化操作。KafkaProducer 会在消息被应答（ Acknowledgement ）之前或消息发送失败时调用生产者拦截器的onAcknowledgement（）方法，优先于用户设定的Callback 之前执行。 

#### 序列化

　　生产者需要用序列化器(Serializer)把对象转换成字节数组才能通过网络发送给Kafka。 而在对侧， 消费者需要用反序列化器(Deserializer)把从Kafka 中收到的字节数组转换成相应的对象。

　　为 了方便， 消息的key和value都使用了字符串， 对应程序中的序列化器也使用了客户端自带的org.apache.kafka. common. serialization. StringSerializer, 除了用于String 类型的序列化器，还有ByteArray、ByteBuffer、 Bytes、 Double、Integer、 Long这几种类型， 它们都实现了org.apache.kafka. common. serialization. Serializer接口



#### 分区器

分区器的作用 就是为消息 分配分区。

消息 经过 序列化 之后就需要确定它发往的分区：

1. 如果消息ProducerRecord中指定了 partitition字段， 那么就不需要分区器的作用， 因为partition代表的就是所要发往的分区号。

2. 如果消息ProducerRecord中没有 指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。 

Kafka 中提供的默认分区器是org.apache.kafka.clients.producer.intemals.DefaultPartitioner, 它实现了org.apache.kafka.clients.producer.Partitioner 接口， 这个接口中定义了2个方法， 具体如下所示。

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);
public void close();
```

其中 partition（）方法用来计算分区号，返回值为 int 类型。partition（）方法中的参数分别表示主题 、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。 close（）方法在关闭分区器的时候用来回收一些资源 。

　　默认的分区器会对key 进行哈希（采用MurmurHash2 算法 ，具备高运算性能及低碰撞率），最终根据得到 的 哈希值来计算分区号， 拥有相同 key 的消息会被写入同一个分区 。 如果 key 为 null ，那么消息将会以轮询的方式发往主题内的各个可用分区。



#### 消息累加器

　　整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程 （发送线程）。

​		在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（ RecordAccumulator，也称为消息收集器〉中。Sender 线程负责从RecordAccumulator 中 获取消息并将其发送到 Kafka 中 。

　　**RecordAccumulator 主要用来缓存消息 以便Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能 。**RecordAccumulator 缓存的大 小可以通过生产者客户端参数buffer. memory 配置，默认值为 33554432B ，即32MB 。 如果生产者发送消息的速度超过发送到服务器的速度 ，则会导致生产者空间不足，这个时候 KafkaProducer 的 send（）方法调用要么被阻塞，要么抛出异常，这个取决于参数 max. block . ms 的配置，此参数的默认值为 6 0000,即 60 秒 。

 　Sender 从RecordAccumulator 中 获取缓存的消息之后，会进一 步将原本＜分区，Deque<Producer Batch＞＞的保存形式转变成＜Node , List< ProducerBatch＞的形式，其中 Node 表示 Kafka集群的 broker 节点 。对于网络连接来说，生产者客户端是与具体 的 broker 节点建立的连接，也就消息累加器

　　整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程 （发送线程）。在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（ RecordAccumulator，也称为消息收集器〉中。Sender 线程负责从RecordAccumulator 中 获取消息并将其发送到 Kafka 中 。

　　**RecordAccumulator 主要用来缓存消息 以便Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能 。**RecordAccumulator 缓存的大 小可以通过生产者客户端参数buffer. memory 配置，默认值为 33554432B ，即32MB 。 如果生产者发送消息的速度超过发送到服务器的速度 ，则会导致生产者空间不足，这个时候 KafkaProducer 的 send（）方法调用要么被阻塞，要么抛出异常，这个取决于参数 max. block . ms 的配置，此参数的默认值为 6 0000,即 60 秒 。

 　Sender 从RecordAccumulator 中 获取缓存的消息之后，会进一 步将原本＜分区，Deque<Producer Batch＞＞的保存形式转变成＜Node , List< ProducerBatch＞的形式，其中 Node 表示 Kafka集群的 broker 节点 。对于网络连接来说，生产者客户端是与具体 的 broker 节点建立的连接，也就是 向具体的broker 节点发送消息，而并不关心消息属于哪一个分区；而对于KafkaProducer的应用逻辑而言 ，我们只 关注向哪个分区中发送哪些消息，所 以在这里需要做一个应用逻辑层面到网络 1/0 层面的转换。

　　元数据是指 Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上，哪些副本在 AR 、ISR 等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。发送哪些消息，所 以在这里需要做一个应用逻辑层面到网络 1/0 层面的转换。

### 重要的生产者参数

　　**1.acks**

​				 这个参数用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的。acks 是生产者客户端中一个非常重要的参数，它涉及消息的可靠性和吞吐量之间的权衡。　　acks 参数有 3 种类型的值（都是字符串类型）。

　　　　acks =1 : 默认值即为l 。生产者发送消息之后，只要分区的leader 副本成功写入消息，那么它就会收到来自服务端的成功响应 。 如果消息无法写入 leader 副本，比如在leader 副本崩溃、重新选举新的leader 副本的过程中，那么生产者就会收到一个错误的响应，为了避免消息丢失，生产者可以选择重发消息 。如果消息写入 leader 副本并返回成功响应给生产者，且在被其他 follower 副本拉取之前 leader 副本崩溃，那么此时消息还是会丢失，因为新选举的 leader 副本中并没有这条对应的消息 。 acks 设置为l ，是消息可靠性和吞吐量之间的折中方案。

　　　　 acks = 0 :生产者发送消 息之后不需要等待任何服务端的响应。如果在消息从发送到写入 Kafka 的过程中出现某些异常，导致 Kafka 并没有收到这条消息，那么生产者也无从得知，消息也就丢失了。在其他配置环境相同的情况下，acks 设置为 0 可以达到最大的吞吐量。

　　　　 acks ＝- l 或 acks =all : 生产者在消 息发送之后，需要等待 ISR 中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应。在其他配置环境相同的情况下，acks 设置为-1(all ）可以达到最强的可靠性。但这并不意味着消息就一定可靠，因为 ISR 中可能只有 leader 副本，这样就退化成了 acks= l 的情况。

　　**2.max.request.size**

　　　　 这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B ，即lMB 。一般情况下，这个默认值就可以满足大多数的应用场景了。

　　**3.retries 和 retry. backoff.ms**

　　　　 retries 参数用来配置生产者重试的次数，默认值为 0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、leader 副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries大于 0 的值，以此通过 内 部重试来恢复而不是一昧地将异常抛给生产者的应用程序。 如果重试达到设定的次数 ，那么生产者就会放弃重试并返回异常。

　　　　不过并不是所有的异常都是可以通过重试来解决的，比如消息太大，超过 max.request.size 参数配置的值时，这种方式就不可行了。 重试还和另一个参数 retry.backoff.ms 有关，这个参数的默认值为100 ，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。在配置 retries 和retry.backoff.ms之前，最好先估算一下可能的异常恢复时间，这样可以设定总的重试时间大于这个异常恢复时间，以此来避免生产者过早地放弃重试 。

　　**4.compression.type**

　　 　这个参数用来指定消息的压缩方式，默认值为“ none ”，即默认情况下，消息不会被压缩。该参数还可以配置为“ gzip ”,“ snappy ” 和“ lz4 ”。 对消息进行压缩可以极大地减少网络传输量 、降低网络 IO ，从而提高整体的性能 。**消息压缩是一种使用时间换空间的优化方式**，如果对时延有一定的要求?，则不推荐对消息进行压缩 。

　　**5. request.timeout.ms**

　　　　 这个参数用来配置 Producer 等待请求响应的最长时间，默认值为 3 0000( ms ）。请求超时之后可以选择进行重试。注意这个参数需要 比 broker 端参数 replica.lag.time.max.ms 的值要大 ，这样可以减少因客户端重试而引起的消息重复的概率。



## 消费者开发

> https://www.cnblogs.com/zjdxr-up/p/16114877.html



### 消费者与消费组

  1. 消费者(Consumer)负责订阅Kafka 中的主题(Topic), 并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是： 在Kafka 的消费理念中还有一层消费组(Consumer Group)的概念， 每一个消费者只隶属于一个消费组。

     ​	•  当消息发布到主题后， 只会被投递给订阅它的每个消费组中的一个消费者。 

     ​	•  且每一个分区只能被一个消费组中的一个消费者所消费。

     ​	•  每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id来配置，默认值为空字符串。

　2. 对于消息中间件而言，一般有两种消息投递模式：点对点(P2P, Point-to-Point)模式和发布／订阅(Pub/ Sub)模式。

​		•  点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。

​		•  发布订阅模式定义了如何向 一个内容节点发布和订阅消息，这个内容节点称为主题(Topic) , 主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布／订阅模式在消息的一对多广播时采用。Kafka 同时支待两种消息投递模式，而这正是得益于消费者与消费组模型的契合：

　　　　 • 如果所有的消费者都隶属于同一个消费组，那么所有的消息就会以均衡地的机会投递给所有消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。

　　 　	• 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布／订阅模式的应用。

　总结：消费组是一个逻辑上的概念，它将旗下的消费者进行归类，来实现消息的点对点、发布等功能。

### 消息消费过程及代码

　　1. 一个正常的消费逻辑需要具备以下几个步骤：

　　　　(1) 配置消费者客户端参数及创建相应的消费者实例。

　　　　(2) 订阅主题。

　　　　(3)拉取消息并消费。

　　　　(4) 提交消费位移。

　　　　(5)关闭消费者实例。

​	2.例
```java
public class KafkaConsumerAnalysis {
    public static final String brokerList = "localhost:9092";
    public static final String topic = "topic-demo";
    public static final String groupid = "group.demo";
    public static final AtomicBoolean isRunning = new AtomicBoolean(true);
    public static Properties initConfig () {
        Properties props= new Properties();
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS—CONFIG,StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
        props.put(ConsumerConfig.GROUP—ID_CONFIG, groupid);
        props. put (ConsumerConfig. CLIENT_ID _ CONFIG, "client. id. demo");
        return props;
    }

    public static void main(String[] args) (
        //1.
        Properties props= initConfig();
        KafkaConsumer<String, String> consumer= new KafkaConsumer<>(props);
        //2.
        //通过 subscribe()方法订阅主题具有 消费者自动再均衡的功能，
        //在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。？
        consumer.subscribe(Arrays.asList(topic));
        try {
            //?
            while (isRunning. get()) {
                //3.
                ConsumerRecords<String, String> records=
                    consumer.poll(Duration.ofMillis(lOOO));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("topic="+record.topic()+ ", partition = "+         record.partition()+ ", offset="+ record.offset());
                    System.out.println("key ="+record.key()+ ", value="+ record.value());
                //do something to process record.
                    //4.?
                }
            }
        } catch(Exception e) {
            log.error("occur exception", e);
        } finally {
            //5.
            consumer.close();
        }
    }
}
```



3.附：

如果我们事先并不知道主题中有多少个分区怎么办,KafkaConsumer中的partitionsFor ()方法可以用来查询指定主题的元数据信息，partitionsFor()方法的具体定义如下： 

```java
public List<Partitioninfo> partitionsFor(String topic)
```

其中 Partitionlnfo类型即为主题的分区元数据信息，此类的主要结构如下：

```
public class Partitioninfo {
	//主题名称
    private final String topic;
    private final int partition;
    //leader代表分区的leader副本所在的位置
    private final Node leader;
    //replicas代表分区的AR集合，inSyncReplicas代表分区的ISR集合，offlineReplicas代表分区的OSR集合。
    private final Node[] replicas;
    private final Node[] inSyncReplicas;
    private final Node[] offlineReplicas;
    ／／这里省略了构造函数、属性提取、toString等方法
}
```

4.？

所以分区与消费组、消费者之间的关系又是什么了，一个分区对应一个消费者，一个分区对应多个消费者

### 消息消费模式

　　　　Kafka中的消费是基于 拉模式的。消息的消费一般有两种模式：推模式和 拉模式。推模式是服务端主动将消息推送给消费者， 而 拉模式是消费者主动向服务端发起请求来拉取消息。Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll()方法，而poll()方法返回的是所订阅的主题（分区）上的一组消息。 对于poll()方法而言，如果某些分区中没有可供消费的消息，那么此分区对应 的消息拉取的结果就为空；如果订阅的所有分区中都没有可供消费的消息， 那么poll()方法返回为空的消息集合

　　　　消费者消费到 的每条消息的类型为ConsumerRecord(注意与ConsumerRecords 的区别，ConsumerRecords为一次获取到的消息集），这个和生产者发送的消息类型ProducerRecord相对应，不过ConsumerRecord中的内容更加丰富，具体的结构参考如下代码：

```
public class ConsumerRecord<K, V> {
    private final Stringtopic;
    private final int partition;
    private final long offset;
    private final long timestamp;
    private final TimestampType timestampType;
    private final int serializedKeySize;
    private final int serializedValueSize;
    private final Headers headers;
    private final K key;
    private final V value;
    private volatile Long checksum;
／／省略若干方法
}
```

　　　　timestarnpType 有两种类型：CreateTime 和LogAppendTime, 分别代表消息创建的时间戳和消息追加到日志的时间戳。





面试：

​	kafka节点之间如何复制备份的？
​	kafka消息是否会丢失？为什么？
​	kafka最合理的配置是什么？
​	kafka的leader选举机制是什么？
​	kafka对硬件的配置有什么要求？
​	kafka的消息保证有几种方式？
​	kafka为什么会丢消息？
​	
​	

	避免消息堆积？
		1） 采用workqueue，多个消费者监听同一队列。
		2）接收到消息以后，而是通过线程池，异步消费。
	如何避免消息丢失？
		1） 消费者的ACK机制。可以防止消费者丢失消息。
		但是，如果在消费者消费之前，MQ就宕机了，消息就没了？
		2）可以将消息进行持久化。要将消息持久化，前提是：队列、Exchange都持久化