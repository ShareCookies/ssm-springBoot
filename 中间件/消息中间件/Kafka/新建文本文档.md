官网：http://kafka.apache.org/
Kafka教程：
	安装：https://www.cnblogs.com/qingyunzong/p/9005062.html
	介绍：
		https://www.cnblogs.com/qingyunzong/p/9004509.html
		https://www.orchome.com/kafka/index
	架构：https://www.cnblogs.com/qingyunzong/p/9004593.html
	高可用：https://www.cnblogs.com/qingyunzong/p/9004703.html
	Kafka在zookeeper中的存储：https://www.cnblogs.com/qingyunzong/p/9007107.html



https://www.cnblogs.com/zjdxr-up/p/16104558.html

## 介绍：

	Kafka是一个分布式消息系统。
	Kafka是一个多分区、多副本且基于 ZooKeeper协调的分布式消息系统。

### 特性：
		目前Kafka已经定位为一个分布式流式处理平台，
		它以高吞吐、
			在一台普通的服务器上既可以达到10W/s的吞吐速率；？
		可持久化、
			快速持久化，可以在O(1)的系统开销下进行消息持久化；？
		可水平扩展、
			完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；
		支持流数据处理等多种特性而被广泛使用。
			？
### 应用：
		1、消息系统：
			Kafka 和传统的消息系统（也称作消息中间件）都具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。与此同时，Kafka 还提供了大多数消息系统难以实现的消息顺序性保障及回溯消费的功能。
		2、存储系统：
			Kafka 把消息持久化到磁盘，相比于其他基于内存存储的系统而言，有效地降低了数据丢失的风险。也正是得益于Kafka 的消息持久化功能和多副本机制，我们可以把Kafka作为长期的数据存储系统来使用，只需要把对应的数据保留策略设置为“永久”或启用主题的日志压缩功能即可。
		3、流式处理平台：
			Kafka 不仅为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、连接、变换和聚合等各类操作
	附：
		Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。
			？为什么rabbitMq称为重量级

## Kafka基本概念：

### Kafka 体系架构：
	1. 一个典型的 Kafka 体系架构包括 ，若干 Producer、若干 Broker、若干 Consumer，以及一个ZooKeeper集群。
	
	2. Producer将消息发送到Broker，Broker负责将收到的消息存储到磁盘中，而Consumer负责从Broker订阅并消费消息
	附：
		其中ZooKeeper是Kafka用来负责集群元数据的管理、控制器的选举等操作的。。
### broker:服务代理节点
	对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务实例。
		附大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。
		一个或多个Broker组成了一个Kafka集群。
		
	broker作用：
		broker 上存储了消息。
#### Topic：主题
		Kafka中的消息以主题为单位进行归类。
		生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。
		附：
			主题是一个逻辑上的概念，它细分为多个分区。一个分区只属于单个主题。
			用户操作消息的时候，只需指定消息的Topic，即可生产或消费数据，而不必关心数据存于何处。
#### Partition：分区
		topic又由为一个或多个partition组成。
		附：主题的分区可以分布在不同的服务器（broker）上。
		附：每一条消息被发送到 broker 之前，会根据分区规则选择存储到哪个具体的分区。 如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中 。 如果一个主题只对应一个文件，那么这个文件所在的机器I/O 将会成为这个主题的性能瓶颈，而分区解决了这个问题 。 在创建主题的时候可以通过指定的参数来设置分区的个数，当然也可以在主题创建完成之后去修改分区的数量，通过增加分区的数量可以实现水平扩展。
##### partition 分区构成：
		分区在存储层面可以看作一个可追加的日志文件。
			偏移量：
				消息在被追加到分区日志文件的时候会分配一个特定的偏移量（offset）。
				offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，
				不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。
		附：
			1. 每个partition中的数据又使用多个segment文件存储。？
			2. 在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。
				如果topic有多个partition，消费数据时就不能保证数据的顺序。
##### partition  副本：
		介绍：
	        Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。
	        同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样）。
	        副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。
	        副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。
		
		follower副本的同步机制：
			消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。(这个范围可以通过参数进行配置。)
			
			AR: 分区中的所有副本统称为AR（Assigned Replicas）。
			ISR:
				所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。ISR与HW和LEO有着紧密的关系。
			OSR: 
				与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR (Out-of-Sync Replicas ），由 此可见， AR=ISR+OSR 。在正常情况下，所有 的 follower 副本都应该与 leader 副本保持一定程度 的同步，即 AR=ISR, OSR 集合为空。leader 副本负 责维护和跟踪 ISR 集合中所有 follower 副本 的滞后状态， 当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除 。 如果 OSR 集合中有 follower 副本 “追上’了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合 。 
			HW： HW是High Watermark的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。
			LEO： LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的offset。	
					
			Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。
			同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。
			？
				那kafka生产端如何判断是成功提交了

![分区中各偏移量说明](http://rfy18y9f3.hn-bkt.clouddn.com/GJ4)5})3HSJ2$$)DRUPS0YF.png)

### Producer 生成者

		生产者即数据的发布者。
		该角色将消息发布到Kafka的topic中。
		附：
			生产者发送的消息，随机存储到一个partition中。当然生产者也可以指定数据存储的partition。
### Consumer 消费者
		消费者可以从broker中读取数据。

## 使用：

### 	客户端开发

#### 	java实现消息发送：

**1. 客户端开发一个正常的生产逻辑需要具备以下几个步骤：**
​	（1）配置生产者客户端参数及创建相应的生产者实例。
​	（2）构建待发送的消息。
​	（3）发送消息。
​	（4）关闭生产者实例。

**2. java kafka进行消息生产发送代码示例：** 

```java
public class KafkaProducerAnalysis {
    public static final String brokerList = "localhost:9092";
    public static final String topic = "topic-demo";
    public static Properties initConfig() (
         Properties props = new Properties();
         props.put("bootstrap.servers", brokerList);
    　　  props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
 　　 　  props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
　　　　  properties. put ("client. id", "producer. client. id. demo");
         return props;
    }
    public static void main(String[] args) {
        Properties props = initConfig();
        //KafkaProducer是线程安全的， 可以在多个线程中共享单个KafkaProducer实例?，也 可以将KafkaProducer实例进行池化来供其他线程调用。
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        ProducerRecord<String,String> record = new ProducerRecord<> (topic, "hello, Kafka1 ");
        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
 }
```



**3. 消息对象ProducerRecord :**

 	构建的消息对象ProducerRecord, 它并不是单纯意义上的消息，它包含了多个属性 

```java
public class ProducerRecord<K, V> {
        private final String topic; //主题
        private final Integer partition; //分区号
    	//headers字段是消息的头部，它大多用来设定 一些与应用相关的信息，如无需要也可以不用设置。
        private final Headers headers; //消息头部
    	//key是用来指定消息的键， 它不仅是消息的附加信息，还可以用来计算分区号进而可以让消息发往特定的分区。
　　　　 //key可以让消息再进行二次归类， 同一个key的消息会被划分到同 一个分区中
    	//有key的消息还可以支持日志压缩的功能
        private final K key; //键
    	//value是指消息体，一般不为空，如果为空则表示特定的消息-墓碑消息;
        private final V value; //值
    	//timestamp是指消息的时间戳，它有 CreateTime 和 LogAppendTime 两种类型，前者表示消息创建的时间，后者表示消息追加到日志文件的时间.
        private final Long timestamp; //消息的时间戳
　　//省略其他成员方法和构造方法
}
```



#### 发送消息的三种模式及实现区别

**1. 发送消息主要有三种模式：** 

​	发后即忘(fire-and-forget)、同步(sync)及异步Casync)。

1. 发后即忘: 它只管往Kafka中发送消息而并不关心消息是否正确到达。在大多数情况下，这种发送方式没有什么问题，不过在某些时候（比如发生不可重试异常时）会造成消息的丢失?。 这种发送方式的性能最高， 可靠性也最差。

2. 同步：

   1. KafkaProducer的 send()方法并非是void类型， 而是Future<RecordMetadata>类型， send()方法有2个重载方法，具体定义如下：

      ```
      public Future<RecordMetadata> send(ProducerRecord<K, V> record)
      public Future<RecordMetadata> send(ProducerRecord<K, V> record,Callback callback)
      ```

   2. 实现同步的发送方式， 可以利用send()返回的 Future 对象实现:


```java
try {
    Future<RecordMetadata> future = producer.send(record);
    //示例中在执行send()方法之后直接链式调用了get()方法来阻塞等待Kaflca的响应，直到消息发送成功， 或者发生异常。 
    RecordMetadata metadata= future.get();
    //返回的RecordMetadata对象里包含了消息的一些元数据信息，比如当前消息的主题、分区号、分区中的偏移量(offset)、 时间戳等。
    System.out.println(metadata.topic() + "-" +metadata.partition() + ":" + metadata.offset());
    } catch (ExecutionException I InterruptedException e) {
    e.printStackTrace () ;
}
```

3. 异步 

   send()方法本身就是异步的

   ```
   public Future<RecordMetadata> send(ProducerRecord<K, V> record,Callback callback)
   ```









面试：
	kafka节点之间如何复制备份的？
	kafka消息是否会丢失？为什么？
	kafka最合理的配置是什么？
	kafka的leader选举机制是什么？
	kafka对硬件的配置有什么要求？
	kafka的消息保证有几种方式？
	kafka为什么会丢消息？
	
	

	避免消息堆积？
		1） 采用workqueue，多个消费者监听同一队列。
		2）接收到消息以后，而是通过线程池，异步消费。
	如何避免消息丢失？
		1） 消费者的ACK机制。可以防止消费者丢失消息。
		但是，如果在消费者消费之前，MQ就宕机了，消息就没了？
		2）可以将消息进行持久化。要将消息持久化，前提是：队列、Exchange都持久化