sql查询优化：
	1. 限定数据的范围.(最基础的)
			务必禁止不带任何限制数据范围条件的查询语句。
			例：
				我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；
				分页等操作。
	2.索引：
		附：
			1.对查询常用字段建立索引，至少提高10多倍速度。
			2.分析，诊断手段：
				./诊断.txt goto:sql问题诊断

	
大表优化
	* https://segmentfault.com/a/1190000006158186
	*待看https://www.zhihu.com/question/19719997/answer/865063773
	sql优化：https://zhuanlan.zhihu.com/p/89793056
	当MySQL单表记录数过大时，数据库的CRUD性能会明显下降.	

	1. 读/写分离
		经典的数据库拆分方案，主库负责写，从库负责读；
		1.数据库实习主从：
			Mysql主从同步（复制）
				https://www.cnblogs.com/kylinlin/p/5258719.html
				https://blog.csdn.net/weixin_30608503/article/details/96199857?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.add_param_isCf&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.add_param_isCf
		2.程序要实现读写分离
			1.代码自己实现：
				https://blog.csdn.net/jackZ01/article/details/81132343?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf

			2.插件：
				sharding-JDBC 实现读写分离：
					https://www.cnblogs.com/boothsun/p/7853526.html
				mycat实现读写分离：
	2.数据库分片：
		./数据库分片.txt
附：
	Mysql高性能优化规范建议：
		https://www.cnblogs.com/huchong/p/10219318.html
	一条SQL语句执行得很慢的原因有哪些：			
		https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485185&idx=1&sn=66ef08b4ab6af5757792223a83fc0d45&chksm=cea248caf9d5c1dc72ec8a281ec16aa3ec3e8066dbb252e27362438a26c33fbe842b0e0adf47&token=79317275&lang=zh_CN#rd
	一条SQL语句在MySQL中如何执行的：
		https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485097&idx=1&sn=84c89da477b1338bdf3e9fcd65514ac1&chksm=cea24962f9d5c074d8d3ff1ab04ee8f0d6486e3d015cfd783503685986485c11738ccb542ba7&token=79317275&lang=zh_CN#rd
？
	百万条数据1秒内
	冗余字段的建立，减少表之间的关联即减少数据的查询量。
		mysql数据以字符串为主的数据库，表数据量极限500万
		mysql数据以整形为主的数据库，表数据量极限1000万
	solr，mongo，redis
	Solr与MySQL查询性能对比：
		https://www.cnblogs.com/luxiaoxun/archive/2015/08/02/4696477.html
		
	加上索引比不加索引还慢：
		https://segmentfault.com/q/1010000014484036/a-1020000014484759	
		对于重复数据很多的列不合适简建立索引，因为过滤后数据量仍然很大，先走索引在走表，所以很慢
