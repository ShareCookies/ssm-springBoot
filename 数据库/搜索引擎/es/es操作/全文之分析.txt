分析（Analysis）与分析器：(分词与分词器)
	分析：
		分析是指下面的过程。
		首先，将一块文本分成适合于倒排索引的独立的词条。
		之后，将这些词条统一化为标准格式以提高它们的“可搜索性”。
			附：标准化是指统一小写、删除特殊符号等
	分析器:
		介绍：
			es使用分析器实现上面的工作。
			分析器实际上是将三个功能封装到了一个包里：
				字符过滤器：(预处理)
					字符过滤器的任务是在分词前整理字符串，字符串按顺序通过每个字符过滤器 。
					例：
						一个字符过滤器可以用来去掉HTML，或者将 & 转化成 and 等。
						<span> hello<span> --> hello

				分词器：(分词)
					分词器会把字符串分为单个的词条。
					例：
						一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。
						hello world --> hello, world
				Token 过滤器：(过滤处理)
					最后词条按顺序通过每个 token 过滤器 。
					这个过程可能会改变词条（例：小写化 Quick ），
					删除词条（例：像 a， and， the 等无用词），
					或者增加词条（例：像 jump 和 leap 这种同义词）。
			附：
				Elasticsearch提供了各种开箱即用的字符过滤器、分词器、token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。
				我们会在 自定义分析器 章节详细讨论。
		内置分析器
			Elasticsearch还附带了可以直接使用的预包装的分析器。
			接下来我们会列出最重要的分析器。
			
			为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：
				"Set the shape to semi-transparent by calling set_trans(5)"
			标准分析器
				标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。
				它会产生
					set, the, shape, to, semi, transparent, by, calling, set_trans, 5
			简单分析器
				简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生
					set, the, shape, to, semi, transparent, by, calling, set, trans
			空格分析器
				空格分析器在空格的地方划分文本。
				它会产生
					Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
			语言分析器
				特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。
				英语 分词器会产生下面的词条：
					set, shape, semi, transpar, call, set_tran, 5
					注意看 transparent、 calling 和 set_trans 已经变为词根格式。	

	分析器什么时候起作用：
		1. 存储时：
			当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。
				附：因为mapping的全文类型要经过分析处理，才可以被全文搜索操作搜索到。

			?
				那些算全文域了，只有text才算全文域吗!
		2. 查询时：
			当你对不同类型的field进行查询时，搜索行为是不一样的，其会跟建立倒排索引的行为保持一致; 	
			1. 当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。
			2. 当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。
				？是去倒排索引查询吗
				？如果是精确值范围查询了
			比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text 搜索时,就会对查询参数进行分词和normalization再 去倒排索中去搜索
			？
				全域查询时(就是不指定field)，查询字符串会被分词，然后是去倒排索引查找吧!
				对指定域进行查询，会根据域类型来决定是否对查询字符串分词？
				如果指定多个域了？
				如果多个参数了！
		待具体案例验证！！！
		例：

			date 域包含一个精确值：单独的词条 2014-09-15。
			_all 域是一个全文域，所以分词进程将日期转化为三个词条： 2014， 09， 和 15。			
			
			url版全文查询：
				1. 当我们在 _all 域查询 2014，它匹配所有的12条推文，因为它们都含有 2014.
					GET /_search?q=2014              # 12 results
	
				2. 当我们在 _all 域查询 2014-09-15，它首先分析查询字符串，产生匹配 2014， 09， 或 15 中 任意 词条的查询。
					这也会匹配所有12条推文，因为它们都含有 2014.
					GET /_search?q=2014-09-15
					
						
			url版精确查询：
				1. 当我们在 date 域查询 2014-09-15，它寻找 精确 日期，只找到一个推文：
					GET /_search?q=date:2014-09-15  	
				
				2. 当我们在 date 域查询 2014，它找不到任何文档，因为没有文档含有这个精确日志：
					GET /_search?q=date:2014

	分析器进阶：
		自定义分析器：
			https://www.yuque.com/yanyulou-jxe2g/iz0eoq/ygoxy0#msRvl
		指定分析器:(为域指定一个分析器)
			1. 当Elasticsearch在你的文档中检测到一个新的字符串域，它会自动设置其为一个全文 字符串 域，并且默认使用的是标准 分析器对它进行分析。

			2. 当然你也可以为新域指定一个分析器，适合于你数据的分析器。
				附：
					指定域的映射可以达到以下效果：
						1. 有时候你想要一个字符串域就是一个字符串域—​不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。
						2.
						...
				例：
				put http://192.168.0.104:9200/索引名/_mapping
				//失败?
				{
				 "properties" : {
				   "textField1" : {
					 "type" :"text",
					 "analyzer": "english"
				   }
				 }
				}


附：			
分析器测试工具：
	使用 analyze API 工具来看文本是如何被分析器分析的。
		便于你理解分词的过程和实际被存储到索引中的词条，有助于我们理解Elasticsearch索引内部发生了什么。
	例：
		在消息体里，指定分析器和要分析的文本：
			GET /_analyze
			{
			  "analyzer": "standard",//指定分析器
			  "text": "Text to analyze"//要分析的文本
			}
		返回值：
			结果中每个元素代表一个单独的词条：
				token：
					token 是实际存储到索引中的词条。 
				position：
					position 指明词条在原始文本中出现的位置。
				start_offset 和 end_offset：
					start_offset 和 end_offset 指明字符在原始字符串中的位置。
					附:
						position是指文本被分词为5个，而当前词条为第几个。
				type：
					每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于​keep_types token 过滤器​。
			{
			   "tokens": [
				  {
					 "token":        "text",
					 "start_offset": 0,
					 "end_offset":   4,
					 "type":         "<ALPHANUM>",
					 "position":     1
				  },
				  {
					 "token":        "to",
					 "start_offset": 5,
					 "end_offset":   7,
					 "type":         "<ALPHANUM>",
					 "position":     2
				  },
				  {
					 "token":        "analyze",
					 "start_offset": 8,
					 "end_offset":   15,
					 "type":         "<ALPHANUM>",
					 "position":     3
				  }
			   ]
			}
